{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Banana Collector solved by a DQN algorithm\n",
    "## Code implementation description\n",
    "The whole code contains four python files:\n",
    "1. `model.py` contains a class __Actor__, which is a neural network (NN) with two hidden layers (37,64,64,4), so this NN codifies the policy.\n",
    "1. `dqn_agent.py` has a class __Agent__ with an __Actor__ class and __ReplayBuffer__ class as members, Agent has three important methods `act`, `step` and `learn`. Furthermore __ReplayBuffer__ is an implementation of the Expirience Replay technique propose in the DQN algorithm.\n",
    "1. `dqn_monitor.py` has a function `dqn_interact` which implements the interacion of the Unity environment and the class __Agent__ following the DQN algorithm.\n",
    "1. `learn_and_prove.py` is the main program and has three options `[-h|--help]` to know how to use the program, `[--train]` to train an agent, and `[--file FILE]` to save the output data from the program for a post-process. \n",
    "\n",
    "## Learning algorithm\n",
    "The __DQN algorithm__ takes the following function to let the Agent learn from experience, here is performed the $\\epsilon_i$ management under GLIE technique for the _exploitation-exploration dilemma_ and a $\\epsilon$-greedy policy function is accomplished under `agent.act(...)`,\n",
    "\n",
    "```python\n",
    "def dqn_interact(env, agent,\n",
    "                 n_episodes=2000, window=100, max_t=1000,\n",
    "                 eps_start=1.0, eps_end=0.005, eps_decay=0.980,\n",
    "                 filename='checkpoint.pth'):\n",
    "    \"\"\" Deep Q-Learning Agent-Environment interaction.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        env: instance of UnityEnvironment class\n",
    "        agent: instance of class Agent (see dqn_agent.py for details)\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        window (int): number of episodes to consider when calculating average rewards\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        filename (string): name of the file to save weights\n",
    "    \"\"\"\n",
    "    # all returns\n",
    "    all_returns = []\n",
    "    # initialize average rewards\n",
    "    avg_rewards = deque(maxlen=n_episodes)\n",
    "    # initialize monitor for most recent rewards\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    # initialize best average reward\n",
    "    best_avg_reward = -np.inf\n",
    "    # initialize eps\n",
    "    eps = eps_start\n",
    "    # for each episode\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # begin the episode\n",
    "        state = reset(env, train_mode=True)\n",
    "        # initialize the sample reward\n",
    "        samp_reward = 0\n",
    "        for t in range(max_t):\n",
    "            # agent selects an action\n",
    "            action =  agent.act(state, eps)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done = step(env, action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # updated the sample reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s-> s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        # save final sampled reward\n",
    "        samp_rewards.append(samp_reward)\n",
    "        all_returns.append(samp_reward)\n",
    "        # update epsion with GLIE\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        # stopping criteria\n",
    "        if np.mean(samp_rewards)>=15.0:\n",
    "            # safe weights\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "```\n",
    "Inside `agent.step(...)` method is carried out the learning process, the agent begin in a [_tabula rasa_](https://en.wikipedia.org/wiki/Tabula_rasa) state, after that each interaction with the environment is saved in the replay buffer and only when the replay buffer is fulfilled with at least `BATCH_SIZE` of experiences the learning process begin with a sample of randomly `BUFFER_SIZE` experiences, this sample is performed in the ReplayBuffer.sample() method.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    " \n",
    "    # ...\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Interact Agent and Environment.\n",
    "        \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step ==0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "```\n",
    "\n",
    "The `learn(...)` method called from `agent.step(...)` performs the TD-update using a target NN as proposed in [this paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) but with a little change in the update step of the target NN using a `soft_update` method proposed in the [paper](https://arxiv.org/pdf/1509.02971.pdf),\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    \n",
    "    # ...\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.actor_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.actor_local(states).gather(1,actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)    \n",
    "```\n",
    "\n",
    "__Hyperparameters:__ The following are the important parameters\n",
    "\n",
    "```python\n",
    "# dqn_interact.py\n",
    "max_t=1000              # maximum number of steps per episode\n",
    "eps_start=1.0           # starting epsilon\n",
    "eps_end=0.005           # final epsilon, holding a bit of exploitation\n",
    "eps_decay=0.980         # epsilon decay\n",
    "# dqn_agent.py\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update or target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "FC1_UNITS = 64          # number of neurons in fisrt layer\n",
    "FC2_UNITS = 64          # number of neurons in second layer\n",
    "```\n",
    "after exporing some of them, the most important found was `eps_decay`.\n",
    "\n",
    "Finally, __the neural networks architecture__ was 37->64->64->4,\n",
    "\n",
    "```python\n",
    "Actor(\n",
    "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
    "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
    "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "the definition can be found in `model.py`, the activation function for hidden layers are `relu` functions,\n",
    "\n",
    "```python\n",
    "class Actor(nn.Module):\n",
    "    \"\"\" Actor (Policy) Model. \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\" Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\" Build a network that maps state -> action values. \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "```\n",
    "\n",
    "## Plots of rewards\n",
    "Reports the number of episodes needed to solve the environment\n",
    "\n",
    "## Ideas for future work\n",
    "The submission has concrete future ideas for improving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnavigation",
   "language": "python",
   "name": "drlnavigation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
