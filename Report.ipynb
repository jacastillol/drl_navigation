{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Banana Collector solved by a DQN algorithm\n",
    "## Code implementation description\n",
    "The whole code contains four python files:\n",
    "1. `model.py` contains a class __Actor__, which is a neural network (NN) with two hidden layers (37,64,64,4), so this NN codifies the policy.\n",
    "1. `dqn_agent.py` has a class __Agent__ with an __Actor__ class and __ReplayBuffer__ class as members, Agent has three important methods `act`, `step` and `learn`. Furthermore __ReplayBuffer__ is an implementation of the Expirience Replay technique propose in the DQN algorithm.\n",
    "1. `dqn_monitor.py` has a function `dqn_interact` which implements the interacion of the Unity environment and the class __Agent__ following the DQN algorithm.\n",
    "1. `learn_and_prove.py` is the main program and has four options `[-h|--help]` to know how to use the program, `[--train]` to train an agent, `[--random]` to run a tabula rasa or a random-policy agent, and `[--file FILE]` to save the output data from the program for a post-process. \n",
    "\n",
    "## Learning algorithm\n",
    "\n",
    "### DQN algorithm explanation\n",
    "The __DQN algorithm__ takes the following function to let the Agent learn from experience, here is performed the $\\epsilon_i$ management under GLIE technique for the _exploitation-exploration dilemma_ and a $\\epsilon$-greedy policy function is accomplished under `agent.act(...)`,\n",
    "\n",
    "```python\n",
    "def dqn_interact(env, agent,\n",
    "                 n_episodes=2000, window=100, max_t=1000,\n",
    "                 eps_start=1.0, eps_end=0.005, eps_decay=0.980,\n",
    "                 filename='checkpoint.pth'):\n",
    "    \"\"\" Deep Q-Learning Agent-Environment interaction.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        env: instance of UnityEnvironment class\n",
    "        agent: instance of class Agent (see dqn_agent.py for details)\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        window (int): number of episodes to consider when calculating average rewards\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        filename (string): name of the file to save weights\n",
    "    \"\"\"\n",
    "    # all returns\n",
    "    all_returns = []\n",
    "    # initialize average rewards\n",
    "    avg_rewards = deque(maxlen=n_episodes)\n",
    "    # initialize monitor for most recent rewards\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    # initialize best average reward\n",
    "    best_avg_reward = -np.inf\n",
    "    # initialize eps\n",
    "    eps = eps_start\n",
    "    # for each episode\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # begin the episode\n",
    "        state = reset(env, train_mode=True)\n",
    "        # initialize the sample reward\n",
    "        samp_reward = 0\n",
    "        for t in range(max_t):\n",
    "            # agent selects an action\n",
    "            action =  agent.act(state, eps)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done = step(env, action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # updated the sample reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s-> s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        # save final sampled reward\n",
    "        samp_rewards.append(samp_reward)\n",
    "        all_returns.append(samp_reward)\n",
    "        # update epsion with GLIE\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        # stopping criteria\n",
    "        if np.mean(samp_rewards)>=15.0:\n",
    "            # safe weights\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "```\n",
    "Inside `agent.step(...)` method is carried out the learning process, the agent begin in a [_tabula rasa_](https://en.wikipedia.org/wiki/Tabula_rasa) state, after that each interaction with the environment is saved in the replay buffer and only when the replay buffer is fulfilled with at least `BATCH_SIZE` of experiences the learning process begin with a sample of randomly `BUFFER_SIZE` experiences, this sample is performed in the ReplayBuffer.sample() method.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    " \n",
    "    # ...\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Interact Agent and Environment.\n",
    "        \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step ==0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "```\n",
    "\n",
    "The `learn(...)` method called from `agent.step(...)` performs the TD-update using a target NN as proposed in [this paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) but with a little change in the update step of the target NN using a `soft_update` method proposed in the [paper](https://arxiv.org/pdf/1509.02971.pdf),\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    \n",
    "    # ...\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.actor_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.actor_local(states).gather(1,actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)    \n",
    "```\n",
    "\n",
    "### Hyperparameters exploration\n",
    "The following important parameters are handled by the following dictionary:\n",
    "\n",
    "```python\n",
    "# current configuration\n",
    "config = {\n",
    "    'n_episodes':       400,  # max. number of episode to train the agent\n",
    "    'window':           100,  # save the last XXX returns of the agent\n",
    "    'max_t':            500,  # max. number of steps per episode\n",
    "    'eps_start':        1.0,  # GLIE parameters\n",
    "    'eps_end':        0.005,  #  - epsilon minimum value\n",
    "    'eps_decay':      0.960,  #  - epsilon decay \n",
    "    'BUFFER_SIZE': int(1e5),  # replay buffer size\n",
    "    'BATCH_SIZE':        16,  # minibatch size\n",
    "    'GAMMA':           0.99,  # discount factor\n",
    "    'TAU':             1e-3,  # for soft update or target parameters\n",
    "    'LR':              5e-4,  # learning rate\n",
    "    'UPDATE_EVERY':       1,  # how often to update the network\n",
    "    'FC1_UNITS':         16,  # number of neurons in fisrt layer\n",
    "    'FC2_UNITS':         16,  # number of neurons in second layer\n",
    "    }\n",
    "```\n",
    "\n",
    "* `n_episodes` was explored from a range $\\in [100-2000]$ and was concluded that 400 episodes was enough.\n",
    "* `max_t` was explored from a range $\\in [100-1000]$ and 500 was chosen because was the minimum amount to let the agent explores close the wall and get that experience without having to wait long episodes.\n",
    "* `eps_decay` was expored with values like $[0.999, 0.99, 0.98, 0.96]$. At the end we pick $\\epsilon_d=0.96$.\n",
    "* `BATCH_SIZE` seems to be not so important. We tried with this values $[16,32,64]$ but nothing important happend, then we chose simplicity.\n",
    "* `GAMMA` was explored with 0.99 and 0.999. It seems better to chose 0.99.\n",
    "* `UPDATE_EVERY` makes learning time to be prolonged.\n",
    "* `FC1_UNITS` and `FC2_UNITS` was explored with values like $[16,32,64]$, finally was chosen 16 for simplicity.\n",
    "\n",
    "The other parameters was not explored. Finally, we conclude the most important parameter found was `eps_decay`.\n",
    "\n",
    "### The neural networks architecture\n",
    "The final architecture for simplicity was 37->16->16->4,\n",
    "\n",
    "```python\n",
    "Actor(\n",
    "  (fc1): Linear(in_features=37, out_features=16, bias=True)\n",
    "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
    "  (fc3): Linear(in_features=16, out_features=4, bias=True)\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "the definition can be found in `model.py`, the activation function for hidden layers are `relu` functions,\n",
    "\n",
    "```python\n",
    "class Actor(nn.Module):\n",
    "    \"\"\" Actor (Policy) Model. \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\" Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\" Build a network that maps state -> action values. \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "```\n",
    "\n",
    "## Plots of rewards\n",
    "Reports the number of episodes needed to solve the environment\n",
    "\n",
    "When the code run the following graph can be build,\n",
    "![title](images/drl01_simple_run.png)\n",
    "all agent's learning seasons have a target of +13 avg score over the last 100 episodes (red line), an upper goal of +15 avg. score over the last 100 episodes (green line) is set in the algorithm to finish the main loop. The grey curve is all the final scores obtained by the 2000 episodes of agent experience. The blue line represents the avg. score over the last 100 scores.\n",
    "\n",
    "The following images show the effect of $\\epsilon_d$ and the reason why could be a good choice of $\\epsilon_d=0.960$; furthermore, `max_t=500` and `n_episodes` will be set for the rest of the analysis. \n",
    "![title](images/drl02_exploring_epsilon_decay.png)\n",
    "![title](images/drl03_how_epsilon_decay.png)\n",
    "\n",
    "The number neurons of the hidden layers was modified and finally simplicity was selected.\n",
    "![title](images/drl04_exploring_fci.png)\n",
    "\n",
    "The stochasticity of the problem makes hard to explore all the paramiters, the following figure has created holding the parameters for five runs and then was obtained the average and standard deviation of episodes to solve the problem.\n",
    "![title](images/drl05_stochasticity_and_gamma.png)\n",
    "```python\n",
    "Episodes solved avg. (avg:323.60 | std:46.81) for epsilon_d = 0.980 and gamma = 0.99\n",
    "Episodes solved avg. (avg:217.20 | std:17.44) for epsilon_d = 0.960 and gamma = 0.99\n",
    "Episodes solved avg. (avg:225.60 | std:14.01) for epsilon_d = 0.960 and gamma = 0.999\n",
    "Episodes solved avg. (avg:234.80 | std:24.77) for epsilon_d = 0.960 and gamma = 0.96\n",
    "```\n",
    "Also the $\\gamma$ effect was explored and its relation with the standard deviation of the learning curve. Nonetheless we thing we need to do more proves.\n",
    "\n",
    "## Ideas for future work\n",
    "\n",
    "For future work its important to make a better systematic search on the parameter space. In the case of DQN algorithm, this can be improved with other improvements suggested in [the paper](https://arxiv.org/abs/1710.02298) with modifications like Double DQN, Prioritized Experience Replay, Dueling DQN and others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnavigation",
   "language": "python",
   "name": "drlnavigation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
